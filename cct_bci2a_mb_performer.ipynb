{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Convolutional Transformer for MI-EEG Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MBPerformerEEGs' from 'model.mb_performer' (/workspace-cloud/seiya.narukawa/EEGCCT/model/mb_performer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcct\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CCT\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmb_performer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MBPerformerEEGs\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchinfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtiny_eegcct_dw\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TinyEEGCCT_DW\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MBPerformerEEGs' from 'model.mb_performer' (/workspace-cloud/seiya.narukawa/EEGCCT/model/mb_performer.py)"
     ]
    }
   ],
   "source": [
    "from model.cct import CCT\n",
    "from model.mb_performer import MBPerformerEEG\n",
    "from torchinfo import summary\n",
    "from model.tiny_eegcct_dw import TinyEEGCCT_DW\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab-shi/anaconda3/envs/eegcct/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import torch \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import mne\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyEEGCCT_DW(num_classes=2,     # 出力クラス（左右 MI なら 2）\n",
    "                      dim=32,            # トークン埋め込み次元\n",
    "                      heads=4,           # マルチヘッド数\n",
    "                      layers=2,          # Encoder ブロック数\n",
    "                      window=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "TinyEEGCCT_DW (TinyEEGCCT_DW)            [64, 1, 22, 1000]    [64, 2]              --                   True\n",
       "├─TinyTokenizer (tokenizer)              [64, 1, 22, 1000]    [64, 5250, 32]       --                   True\n",
       "│    └─Sequential (conv)                 [64, 1, 22, 1000]    [64, 32, 21, 250]    --                   True\n",
       "│    │    └─DWConvBlock (0)              [64, 1, 22, 1000]    [64, 16, 21, 1000]   70                   True\n",
       "│    │    └─DWConvBlock (1)              [64, 16, 21, 1000]   [64, 32, 21, 1000]   976                  True\n",
       "│    │    └─AvgPool2d (2)                [64, 32, 21, 1000]   [64, 32, 21, 250]    --                   --\n",
       "│    │    └─DWConvBlock (3)              [64, 32, 21, 250]    [64, 32, 21, 250]    1,888                True\n",
       "│    └─Flatten (flatten)                 [64, 32, 21, 250]    [64, 32, 5250]       --                   --\n",
       "├─Sequential (encoder)                   [64, 5250, 32]       [64, 5250, 32]       --                   True\n",
       "│    └─EncoderBlock (0)                  [64, 5250, 32]       [64, 5250, 32]       --                   True\n",
       "│    │    └─LayerNorm (norm1)            [64, 5250, 32]       [64, 5250, 32]       64                   True\n",
       "│    │    └─WindowAttention (attn)       [64, 5250, 32]       [64, 5250, 32]       4,128                True\n",
       "│    │    └─LayerNorm (norm2)            [64, 5250, 32]       [64, 5250, 32]       64                   True\n",
       "│    │    └─Sequential (mlp)             [64, 5250, 32]       [64, 5250, 32]       4,192                True\n",
       "│    └─EncoderBlock (1)                  [64, 5250, 32]       [64, 5250, 32]       --                   True\n",
       "│    │    └─LayerNorm (norm1)            [64, 5250, 32]       [64, 5250, 32]       64                   True\n",
       "│    │    └─WindowAttention (attn)       [64, 5250, 32]       [64, 5250, 32]       4,128                True\n",
       "│    │    └─LayerNorm (norm2)            [64, 5250, 32]       [64, 5250, 32]       64                   True\n",
       "│    │    └─Sequential (mlp)             [64, 5250, 32]       [64, 5250, 32]       4,192                True\n",
       "├─LayerNorm (norm)                       [64, 5250, 32]       [64, 5250, 32]       64                   True\n",
       "├─Linear (pool)                          [64, 5250, 32]       [64, 5250, 1]        33                   True\n",
       "├─Linear (fc)                            [64, 32]             [64, 2]              66                   True\n",
       "========================================================================================================================\n",
       "Total params: 19,993\n",
       "Trainable params: 19,993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.98\n",
       "========================================================================================================================\n",
       "Input size (MB): 5.63\n",
       "Forward/backward pass size (MB): 3113.95\n",
       "Params size (MB): 0.08\n",
       "Estimated Total Size (MB): 3119.66\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model,\n",
    "        input_size=(64, 1, 22, 1000),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A01.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A02.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A03.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A04.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A05.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A06.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A07.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A08.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A09.pkl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from a file.\n",
    "    :param filename: Path to the data file.\n",
    "    :return: Loaded data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# ① ファイルパスリストから被験者ごとの dict をまとめて読み込む\n",
    "datasets = sorted(glob.glob(\"pickles/A*.pkl\"))\n",
    "all_subjects = [load_data(fn) for fn in datasets]\n",
    "\n",
    "# ② subject=0 の辞書を取ってくる\n",
    "subj0 = all_subjects[0]\n",
    "\n",
    "# ③ train セッションのデータ\n",
    "train = subj0[\"train\"]\n",
    "X_train = train[\"X\"]          # shape=(288,22,1000)\n",
    "y_train = train[\"y\"]          # label array (0 or 1、artifactマスク等でフィルタ)\n",
    "\n",
    "# ④ eval セッションのデータ\n",
    "eval_ = subj0[\"eval\"]\n",
    "X_eval = eval_[\"X\"]\n",
    "y_eval = eval_[\"y\"]           # None のはず\n",
    "\n",
    "# ⑤ メタ情報\n",
    "meta = subj0[\"meta\"]\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('Your GPU device name :', torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of Parameters and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(test_sub, val_sub, n_subj=9):\n",
    "    \"\"\"\n",
    "    Initialize parameters, model, and loss functions.\n",
    "    :param test_sub: Index of the test subject.\n",
    "    :param val_sub: Index of the validation subject.\n",
    "    :param n_subj: Total number of subjects.\n",
    "    :return: Initialized model and parameters.\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        'batch_size': 32,\n",
    "        'n_epochs': 100,\n",
    "        'lr': 3e-5,\n",
    "        'b1': 0.9,\n",
    "        'b2': 0.999,\n",
    "        'test_Sub': test_sub,\n",
    "        'val_Sub': val_sub,\n",
    "        'n_subjects': n_subj\n",
    "    }\n",
    "\n",
    "    model = CCT(kernel_sizes=[(22, 1), (1, 24)], stride=(1, 1), padding=(0, 0),\n",
    "                pooling_kernel_size=(3, 3), pooling_stride=(1, 1), pooling_padding=(0, 0),\n",
    "                n_conv_layers=2, n_input_channels=1, in_planes=64, activation=None,  # ReLU\n",
    "                max_pool=False, conv_bias=False, dim=64, num_layers=3, num_heads=4, num_classes=2,\n",
    "                attn_dropout=0.1, dropout=0.1, mlp_size=64, positional_emb=\"learnable\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    loss_functions = {\n",
    "        'criterion_l1': nn.L1Loss().cuda(),\n",
    "        'criterion_l2': nn.MSELoss().cuda(),\n",
    "        'criterion_cls': nn.CrossEntropyLoss().cuda()\n",
    "    }\n",
    "\n",
    "    return model, parameters, loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_data(test_sub, val_sub, n_subj=9):\n",
    "    # １）全被験者ロード\n",
    "    all_data = [load_data(fn) for fn in datasets]\n",
    "    \n",
    "    # ２）テスト／バリデーション辞書\n",
    "    test_d = all_data[test_sub]['train']\n",
    "    val_d  = all_data[val_sub] ['train']\n",
    "    \n",
    "    # ３）残りでトレイン\n",
    "    train_idxs = [i for i in range(n_subj) if i not in (test_sub,val_sub)]\n",
    "    train_ds   = [all_data[i]['train'] for i in train_idxs]\n",
    "    \n",
    "    # ４）X,y をそれぞれ取り出して連結\n",
    "    X_train = np.concatenate([d['X'] for d in train_ds], axis=0)\n",
    "    y_train = np.concatenate([d['y'] for d in train_ds], axis=0)\n",
    "    X_val   = val_d ['X']\n",
    "    y_val   = val_d ['y']\n",
    "    X_test  = test_d['X']\n",
    "    y_test  = test_d['y']\n",
    "    \n",
    "    # ５）２クラスフィルタリング：left(0), right(1) のみ残す\n",
    "    mask_tr = np.isin(y_train, [0,1])\n",
    "    X_train, y_train = X_train[mask_tr], y_train[mask_tr]\n",
    "    mask_val = np.isin(y_val, [0,1])\n",
    "    X_val,   y_val   = X_val[mask_val],   y_val[mask_val]\n",
    "    mask_te  = np.isin(y_test, [0,1])\n",
    "    X_test,  y_test  = X_test[mask_te],  y_test[mask_te]\n",
    "\n",
    "    # ６）あとは既存の次元展開／シャッフル／標準化…\n",
    "    X_train = np.expand_dims(X_train,1)\n",
    "    X_val   = np.expand_dims(X_val,  1)\n",
    "    X_test  = np.expand_dims(X_test, 1)\n",
    "\n",
    "    # shuffle train\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_train, y_train = X_train[idx], y_train[idx]\n",
    "\n",
    "    # standardize based on train set\n",
    "    μ, σ = X_train.mean(), X_train.std()\n",
    "    X_train = (X_train - μ) / σ\n",
    "    X_val   = (X_val   - μ) / σ\n",
    "    X_test  = (X_test  - μ) / σ\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(X_train, y_train, X_val, y_val, batch_size):\n",
    "    \"\"\"\n",
    "    Convert numpy arrays to PyTorch tensors and prepare DataLoaders for training and validation.\n",
    "    :param X_train: Training data (numpy array).\n",
    "    :param y_train: Training labels (numpy array).\n",
    "    :param X_val: Validation data (numpy array).\n",
    "    :param y_val: Validation labels (numpy array).\n",
    "    :param batch_size: Batch size for the DataLoader.\n",
    "    :return: DataLoaders for training and validation.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to Tensors\n",
    "    train_data = torch.from_numpy(X_train).type(torch.cuda.FloatTensor)\n",
    "    train_labels = torch.from_numpy(y_train).type(torch.cuda.LongTensor)\n",
    "    val_data = torch.from_numpy(X_val).type(torch.cuda.FloatTensor)\n",
    "    val_labels = torch.from_numpy(y_val).type(torch.cuda.LongTensor)\n",
    "\n",
    "    # Prepare DataLoader for training data\n",
    "    train_dataset = TensorDataset(train_data, train_labels)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Prepare DataLoader for validation data\n",
    "    val_dataset = TensorDataset(val_data, val_labels)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, batch_size, n_segments=3):\n",
    "    # X: (n_trials,1,22,1000)\n",
    "    n_trials,_,n_ch,n_t = X.shape\n",
    "    half = batch_size // 2\n",
    "\n",
    "    # 1000 samples を n_segments 等分する境界を計算\n",
    "    bounds = [ int(round(i * n_t / n_segments)) for i in range(n_segments+1) ]\n",
    "    # e.g. bounds = [0, 333, 667, 1000]\n",
    "\n",
    "    aug_data  = np.zeros((half,1,n_ch,n_t), dtype=X.dtype)\n",
    "    aug_label = np.zeros(half,      dtype=y.dtype)\n",
    "\n",
    "    classes = [0,1]  # 左手/右手 のラベル\n",
    "\n",
    "    for i in range(half):\n",
    "        lbl = np.random.choice(classes)\n",
    "        aug_label[i] = lbl\n",
    "\n",
    "        # 各セグメントごとに同クラスからランダムに試行を選ぶ\n",
    "        idxs = np.where(y == lbl)[0]\n",
    "        picks = np.random.choice(idxs, size=n_segments, replace=True)\n",
    "\n",
    "        segments = []\n",
    "        for s in range(n_segments):\n",
    "            st, ed = bounds[s], bounds[s+1]\n",
    "            segments.append(X[picks[s], 0, :, st:ed])\n",
    "\n",
    "        # 再度連結して長さ 1000 に戻す\n",
    "        new_trial = np.concatenate(segments, axis=-1)  # (22,1000)\n",
    "        aug_data[i,0] = new_trial\n",
    "\n",
    "    # Tensor 化して GPU へ\n",
    "    tdata   = torch.from_numpy(aug_data).float().cuda()\n",
    "    tlabels = torch.from_numpy(aug_label).long().cuda()\n",
    "    return tdata, tlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initialize the EarlyStopping object.\n",
    "        :param patience: Number of epochs to wait after min has been hit. After this number, training stops.\n",
    "        :param min_delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_functions, train_loader, val_loader, parameters, X_train, y_train, early_stopping):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    :param model: The neural network model to train.\n",
    "    :param optimizer: Optimizer for the model.\n",
    "    :param criterion_cls: Loss function for classification.\n",
    "    :param train_loader: DataLoader for training data.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param n_epochs: Number of epochs to train the model.\n",
    "    :return: Trained model.\n",
    "    \"\"\"\n",
    "    # Lists to keep track of metrics\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(parameters['n_epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            # Data augmentation\n",
    "            aug_images, aug_labels = augment_data(X_train, y_train, parameters['batch_size'])\n",
    "            images = torch.cat((images, aug_images))\n",
    "            labels = torch.cat((labels, aug_labels))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_functions['criterion_cls'](outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = loss_functions['criterion_cls'](outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            # Calculate average losses and accuracy\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print(f'Epoch [{epoch+1}/{parameters[\"n_epochs\"]}], Train Loss: {loss.item():.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "            \n",
    "        # Append metrics to lists\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loss_functions, test_loader):\n",
    "    \"\"\"\n",
    "    Test the model using the test dataset.\n",
    "    :param model: The trained neural network model.\n",
    "    :param criterion_cls: Loss function for classification.\n",
    "    :param test_loader: DataLoader for test data.\n",
    "    :return: Test accuracy and test loss.\n",
    "    \"\"\"\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_functions['criterion_cls'](outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have n subjects in your dataset\n",
    "n_subjects = 9\n",
    "\n",
    "# Results storage\n",
    "all_test_accuracies = []\n",
    "all_test_losses = []\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "results_df = pd.DataFrame(columns=['Test Subject', 'Val Subject', 'Test Acc', 'Seed'])\n",
    "\n",
    "for test_sub in range(n_subjects):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    seed_n = np.random.randint(2021)\n",
    "    print('seed is ' + str(seed_n))\n",
    "    random.seed(seed_n)\n",
    "    np.random.seed(seed_n)\n",
    "    torch.manual_seed(seed_n)\n",
    "    torch.cuda.manual_seed(seed_n)\n",
    "    torch.cuda.manual_seed_all(seed_n)\n",
    "    \n",
    "    # Selecting the validation subject (can be the same or different from the test subject)\n",
    "    val_sub = (test_sub + 1) % n_subjects\n",
    "    print(f\"Val Subject {val_sub + 1}:\")\n",
    "\n",
    "    # Initialize model and get source data for this iteration\n",
    "    model, parameters, loss_functions = initialize_model(test_sub, val_sub, n_subjects)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = get_source_data(test_sub, val_sub, n_subjects)\n",
    "    train_loader, val_loader = prepare_dataloaders(X_train, y_train, X_val, y_val, parameters['batch_size'])\n",
    "    test_loader = prepare_dataloaders(X_test, y_test, X_test, y_test, parameters['batch_size'])[1]  # Only need test loader\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters['lr'], betas=(parameters['b1'], parameters['b2']))\n",
    "\n",
    "    # Train the model\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
    "    trained_model, train_losses, val_losses, val_accuracies = train_model(model, optimizer, loss_functions, train_loader, val_loader, parameters, X_train, y_train, early_stopping)\n",
    "    \n",
    "    # Number of epochs trained is either the total number of epochs or until early stopping\n",
    "    epochs_trained = parameters['n_epochs'] if not early_stopping.early_stop else early_stopping.counter\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Test the model\n",
    "    test_accuracy, test_loss = test_model(trained_model, loss_functions, test_loader)\n",
    "\n",
    "    # Store results\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "    all_test_losses.append(test_loss)\n",
    "\n",
    "    print(f\"Test Subject {test_sub + 1}: Test Acc = {test_accuracy:.2f}%, Test Loss = {test_loss:.4f}\")\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n",
    "    print('\\n======================================')\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results_df.loc[len(results_df)] = {\n",
    "        'Test Subject': test_sub + 1,\n",
    "        'Val Subject' : val_sub + 1,\n",
    "        'Test Acc'    : test_accuracy,\n",
    "        'Seed'        : seed_n\n",
    "}\n",
    "\n",
    "# Calculate average performance across all LOSO rounds\n",
    "average_accuracy = np.mean(all_test_accuracies)\n",
    "average_loss = np.mean(all_test_losses)\n",
    "\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.2f}%\")\n",
    "print(f\"Average Test Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "    :param model: The trained model.\n",
    "    :param path: File path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Load a model from a file.\n",
    "    :param path: File path to the model.\n",
    "    :return: Loaded model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = model_class(*args, **kwargs)\n",
    "    \n",
    "    # Load the model state dict\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(trained_model, 'results_2024_conf/model_cct.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loaded_model = load_model('results_2024_conf/model_cct.pth', ['left_hand', 'right_hand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegcct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
