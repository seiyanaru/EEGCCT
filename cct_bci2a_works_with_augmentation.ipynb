{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Convolutional Transformer for MI-EEG Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/islab-shi/anaconda3/envs/eegcct/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.cct import CCT\n",
    "from torchinfo import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import mne\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CCT(kernel_sizes=[(22, 1), (1, 24)], stride=(1, 1), padding=(0, 0),\n",
    "            pooling_kernel_size=(3, 3), pooling_stride=(1, 1), pooling_padding=(0, 0),\n",
    "            n_conv_layers=2, n_input_channels=1,\n",
    "            in_planes=64, activation=None, # ReLU\n",
    "            max_pool=False, conv_bias=False,\n",
    "            dim=64, num_layers=3,\n",
    "            num_heads=4, num_classes=2, \n",
    "            attn_dropout=0.1, dropout=0.1, \n",
    "            mlp_size=64, positional_emb=\"learnable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "CCT (CCT)                                          [64, 1, 22, 1000]    [64, 2]              --                   True\n",
       "├─Tokenizer (tokenizer)                            [64, 1, 22, 1000]    [64, 977, 64]        --                   True\n",
       "│    └─Sequential (conv_layers)                    [64, 1, 22, 1000]    [64, 64, 1, 977]     --                   True\n",
       "│    │    └─Sequential (0)                         [64, 1, 22, 1000]    [64, 64, 1, 1000]    1,408                True\n",
       "│    │    └─Sequential (1)                         [64, 64, 1, 1000]    [64, 64, 1, 977]     98,304               True\n",
       "│    └─Flatten (flattener)                         [64, 64, 1, 977]     [64, 64, 977]        --                   --\n",
       "├─Transformer (transformer)                        [64, 977, 64]        [64, 2]              62,528               True\n",
       "│    └─Dropout (dropout)                           [64, 977, 64]        [64, 977, 64]        --                   --\n",
       "│    └─ModuleList (blocks)                         --                   --                   --                   True\n",
       "│    │    └─EncoderLayer (0)                       [64, 977, 64]        [64, 977, 64]        24,896               True\n",
       "│    │    └─EncoderLayer (1)                       [64, 977, 64]        [64, 977, 64]        24,896               True\n",
       "│    │    └─EncoderLayer (2)                       [64, 977, 64]        [64, 977, 64]        24,896               True\n",
       "│    └─LayerNorm (norm)                            [64, 977, 64]        [64, 977, 64]        128                  True\n",
       "│    └─Linear (attention_pool)                     [64, 977, 64]        [64, 977, 1]         65                   True\n",
       "│    └─Linear (fc)                                 [64, 64]             [64, 2]              130                  True\n",
       "==================================================================================================================================\n",
       "Total params: 237,251\n",
       "Trainable params: 237,251\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.24\n",
       "==================================================================================================================================\n",
       "Input size (MB): 5.63\n",
       "Forward/backward pass size (MB): 769.60\n",
       "Params size (MB): 0.70\n",
       "Estimated Total Size (MB): 775.93\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model,\n",
    "        input_size=(64, 1, 22, 1000),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A01.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A02.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A03.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A04.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A05.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A06.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A07.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A08.pkl',\n",
    "    '/workspace-cloud/seiya.narukawa/EEGCCT/pickles/A09.pkl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from a file.\n",
    "    :param filename: Path to the data file.\n",
    "    :return: Loaded data.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'A01', 'sfreq': 250.0, 'ch_names': ['EEG-Fz', 'EEG-0', 'EEG-1', 'EEG-2', 'EEG-3', 'EEG-4', 'EEG-5', 'EEG-C3', 'EEG-6', 'EEG-Cz', 'EEG-7', 'EEG-C4', 'EEG-8', 'EEG-9', 'EEG-10', 'EEG-11', 'EEG-12', 'EEG-13', 'EEG-14', 'EEG-Pz', 'EEG-15', 'EEG-16'], 'label_map': {'left': 0, 'right': 1, 'feet': 2, 'tongue': 3}, 'window': '0.0–4.0 s (cue-locked)'}\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# ① ファイルパスリストから被験者ごとの dict をまとめて読み込む\n",
    "datasets = sorted(glob.glob(\"pickles/A*.pkl\"))\n",
    "all_subjects = [load_data(fn) for fn in datasets]\n",
    "\n",
    "# ② subject=0 の辞書を取ってくる\n",
    "subj0 = all_subjects[0]\n",
    "\n",
    "# ③ train セッションのデータ\n",
    "train = subj0[\"train\"]\n",
    "X_train = train[\"X\"]          # shape=(288,22,1000)\n",
    "y_train = train[\"y\"]          # label array (0 or 1、artifactマスク等でフィルタ)\n",
    "\n",
    "# ④ eval セッションのデータ\n",
    "eval_ = subj0[\"eval\"]\n",
    "X_eval = eval_[\"X\"]\n",
    "y_eval = eval_[\"y\"]           # None のはず\n",
    "\n",
    "# ⑤ メタ情報\n",
    "meta = subj0[\"meta\"]\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your GPU device name : NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('Your GPU device name :', torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of Parameters and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(test_sub, val_sub, n_subj=9):\n",
    "    \"\"\"\n",
    "    Initialize parameters, model, and loss functions.\n",
    "    :param test_sub: Index of the test subject.\n",
    "    :param val_sub: Index of the validation subject.\n",
    "    :param n_subj: Total number of subjects.\n",
    "    :return: Initialized model and parameters.\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        'batch_size': 32,\n",
    "        'n_epochs': 100,\n",
    "        'lr': 3e-5,\n",
    "        'b1': 0.9,\n",
    "        'b2': 0.999,\n",
    "        'test_Sub': test_sub,\n",
    "        'val_Sub': val_sub,\n",
    "        'n_subjects': n_subj\n",
    "    }\n",
    "\n",
    "    model = CCT(kernel_sizes=[(22, 1), (1, 24)], stride=(1, 1), padding=(0, 0),\n",
    "                pooling_kernel_size=(3, 3), pooling_stride=(1, 1), pooling_padding=(0, 0),\n",
    "                n_conv_layers=2, n_input_channels=1, in_planes=64, activation=None,  # ReLU\n",
    "                max_pool=False, conv_bias=False, dim=64, num_layers=3, num_heads=4, num_classes=2,\n",
    "                attn_dropout=0.1, dropout=0.1, mlp_size=64, positional_emb=\"learnable\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    loss_functions = {\n",
    "        'criterion_l1': nn.L1Loss().cuda(),\n",
    "        'criterion_l2': nn.MSELoss().cuda(),\n",
    "        'criterion_cls': nn.CrossEntropyLoss().cuda()\n",
    "    }\n",
    "\n",
    "    return model, parameters, loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_data(test_sub, val_sub, n_subj=9):\n",
    "    # １）全被験者ロード\n",
    "    all_data = [load_data(fn) for fn in datasets]\n",
    "    \n",
    "    # ２）テスト／バリデーション辞書\n",
    "    test_d = all_data[test_sub]['train']\n",
    "    val_d  = all_data[val_sub] ['train']\n",
    "    \n",
    "    # ３）残りでトレイン\n",
    "    train_idxs = [i for i in range(n_subj) if i not in (test_sub,val_sub)]\n",
    "    train_ds   = [all_data[i]['train'] for i in train_idxs]\n",
    "    \n",
    "    # ４）X,y をそれぞれ取り出して連結\n",
    "    X_train = np.concatenate([d['X'] for d in train_ds], axis=0)\n",
    "    y_train = np.concatenate([d['y'] for d in train_ds], axis=0)\n",
    "    X_val   = val_d ['X']\n",
    "    y_val   = val_d ['y']\n",
    "    X_test  = test_d['X']\n",
    "    y_test  = test_d['y']\n",
    "    \n",
    "    # ５）２クラスフィルタリング：left(0), right(1) のみ残す\n",
    "    mask_tr = np.isin(y_train, [0,1])\n",
    "    X_train, y_train = X_train[mask_tr], y_train[mask_tr]\n",
    "    mask_val = np.isin(y_val, [0,1])\n",
    "    X_val,   y_val   = X_val[mask_val],   y_val[mask_val]\n",
    "    mask_te  = np.isin(y_test, [0,1])\n",
    "    X_test,  y_test  = X_test[mask_te],  y_test[mask_te]\n",
    "\n",
    "    # ６）あとは既存の次元展開／シャッフル／標準化…\n",
    "    X_train = np.expand_dims(X_train,1)\n",
    "    X_val   = np.expand_dims(X_val,  1)\n",
    "    X_test  = np.expand_dims(X_test, 1)\n",
    "\n",
    "    # shuffle train\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_train, y_train = X_train[idx], y_train[idx]\n",
    "\n",
    "    # standardize based on train set\n",
    "    μ, σ = X_train.mean(), X_train.std()\n",
    "    X_train = (X_train - μ) / σ\n",
    "    X_val   = (X_val   - μ) / σ\n",
    "    X_test  = (X_test  - μ) / σ\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(X_train, y_train, X_val, y_val, batch_size):\n",
    "    \"\"\"\n",
    "    Convert numpy arrays to PyTorch tensors and prepare DataLoaders for training and validation.\n",
    "    :param X_train: Training data (numpy array).\n",
    "    :param y_train: Training labels (numpy array).\n",
    "    :param X_val: Validation data (numpy array).\n",
    "    :param y_val: Validation labels (numpy array).\n",
    "    :param batch_size: Batch size for the DataLoader.\n",
    "    :return: DataLoaders for training and validation.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to Tensors\n",
    "    train_data = torch.from_numpy(X_train).type(torch.cuda.FloatTensor)\n",
    "    train_labels = torch.from_numpy(y_train).type(torch.cuda.LongTensor)\n",
    "    val_data = torch.from_numpy(X_val).type(torch.cuda.FloatTensor)\n",
    "    val_labels = torch.from_numpy(y_val).type(torch.cuda.LongTensor)\n",
    "\n",
    "    # Prepare DataLoader for training data\n",
    "    train_dataset = TensorDataset(train_data, train_labels)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Prepare DataLoader for validation data\n",
    "    val_dataset = TensorDataset(val_data, val_labels)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X, y, batch_size, n_segments=3):\n",
    "    # X: (n_trials,1,22,1000)\n",
    "    n_trials,_,n_ch,n_t = X.shape\n",
    "    half = batch_size // 2\n",
    "\n",
    "    # 1000 samples を n_segments 等分する境界を計算\n",
    "    bounds = [ int(round(i * n_t / n_segments)) for i in range(n_segments+1) ]\n",
    "    # e.g. bounds = [0, 333, 667, 1000]\n",
    "\n",
    "    aug_data  = np.zeros((half,1,n_ch,n_t), dtype=X.dtype)\n",
    "    aug_label = np.zeros(half,      dtype=y.dtype)\n",
    "\n",
    "    classes = [0,1]  # 左手/右手 のラベル\n",
    "\n",
    "    for i in range(half):\n",
    "        lbl = np.random.choice(classes)\n",
    "        aug_label[i] = lbl\n",
    "\n",
    "        # 各セグメントごとに同クラスからランダムに試行を選ぶ\n",
    "        idxs = np.where(y == lbl)[0]\n",
    "        picks = np.random.choice(idxs, size=n_segments, replace=True)\n",
    "\n",
    "        segments = []\n",
    "        for s in range(n_segments):\n",
    "            st, ed = bounds[s], bounds[s+1]\n",
    "            segments.append(X[picks[s], 0, :, st:ed])\n",
    "\n",
    "        # 再度連結して長さ 1000 に戻す\n",
    "        new_trial = np.concatenate(segments, axis=-1)  # (22,1000)\n",
    "        aug_data[i,0] = new_trial\n",
    "\n",
    "    # Tensor 化して GPU へ\n",
    "    tdata   = torch.from_numpy(aug_data).float().cuda()\n",
    "    tlabels = torch.from_numpy(aug_label).long().cuda()\n",
    "    return tdata, tlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initialize the EarlyStopping object.\n",
    "        :param patience: Number of epochs to wait after min has been hit. After this number, training stops.\n",
    "        :param min_delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_functions, train_loader, val_loader, parameters, X_train, y_train, early_stopping):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    :param model: The neural network model to train.\n",
    "    :param optimizer: Optimizer for the model.\n",
    "    :param criterion_cls: Loss function for classification.\n",
    "    :param train_loader: DataLoader for training data.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param n_epochs: Number of epochs to train the model.\n",
    "    :return: Trained model.\n",
    "    \"\"\"\n",
    "    # Lists to keep track of metrics\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(parameters['n_epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            # Data augmentation\n",
    "            aug_images, aug_labels = augment_data(X_train, y_train, parameters['batch_size'])\n",
    "            images = torch.cat((images, aug_images))\n",
    "            labels = torch.cat((labels, aug_labels))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_functions['criterion_cls'](outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in val_loader:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "                outputs = model(images)\n",
    "                loss = loss_functions['criterion_cls'](outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "            # Calculate average losses and accuracy\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print(f'Epoch [{epoch+1}/{parameters[\"n_epochs\"]}], Train Loss: {loss.item():.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "            \n",
    "        # Append metrics to lists\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model, train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loss_functions, test_loader):\n",
    "    \"\"\"\n",
    "    Test the model using the test dataset.\n",
    "    :param model: The trained neural network model.\n",
    "    :param criterion_cls: Loss function for classification.\n",
    "    :param test_loader: DataLoader for test data.\n",
    "    :return: Test accuracy and test loss.\n",
    "    \"\"\"\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_functions['criterion_cls'](outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed is 1334\n",
      "Val Subject 2:\n",
      "Epoch [1/100], Train Loss: 0.6918, Val Acc: 54.17%\n",
      "Epoch [2/100], Train Loss: 0.6933, Val Acc: 50.69%\n",
      "Epoch [3/100], Train Loss: 0.6917, Val Acc: 50.69%\n",
      "Epoch [4/100], Train Loss: 0.6883, Val Acc: 51.39%\n",
      "Epoch [5/100], Train Loss: 0.6939, Val Acc: 50.00%\n",
      "Epoch [6/100], Train Loss: 0.6927, Val Acc: 61.81%\n",
      "Epoch [7/100], Train Loss: 0.6943, Val Acc: 52.78%\n",
      "Epoch [8/100], Train Loss: 0.7006, Val Acc: 52.78%\n",
      "Epoch [9/100], Train Loss: 0.7267, Val Acc: 51.39%\n",
      "Epoch [10/100], Train Loss: 0.6987, Val Acc: 57.64%\n",
      "Epoch [11/100], Train Loss: 0.7052, Val Acc: 61.81%\n",
      "Epoch [12/100], Train Loss: 0.7031, Val Acc: 56.94%\n",
      "Epoch [13/100], Train Loss: 0.7370, Val Acc: 54.86%\n",
      "Epoch [14/100], Train Loss: 0.7575, Val Acc: 56.94%\n",
      "Epoch [15/100], Train Loss: 0.8173, Val Acc: 59.72%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m trained_model, train_losses, val_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Number of epochs trained is either the total number of epochs or until early stopping\u001b[39;00m\n\u001b[1;32m     39\u001b[0m epochs_trained \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m early_stopping\u001b[38;5;241m.\u001b[39mearly_stop \u001b[38;5;28;01melse\u001b[39;00m early_stopping\u001b[38;5;241m.\u001b[39mcounter\n",
      "Cell \u001b[0;32mIn[17], line 38\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, loss_functions, train_loader, val_loader, parameters, X_train, y_train, early_stopping)\u001b[0m\n\u001b[1;32m     35\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 38\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Validation accuracy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have n subjects in your dataset\n",
    "n_subjects = 9\n",
    "\n",
    "# Results storage\n",
    "all_test_accuracies = []\n",
    "all_test_losses = []\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "results_df = pd.DataFrame(columns=['Test Subject', 'Val Subject', 'Test Acc', 'Seed'])\n",
    "\n",
    "for test_sub in range(n_subjects):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    seed_n = np.random.randint(2021)\n",
    "    print('seed is ' + str(seed_n))\n",
    "    random.seed(seed_n)\n",
    "    np.random.seed(seed_n)\n",
    "    torch.manual_seed(seed_n)\n",
    "    torch.cuda.manual_seed(seed_n)\n",
    "    torch.cuda.manual_seed_all(seed_n)\n",
    "    \n",
    "    # Selecting the validation subject (can be the same or different from the test subject)\n",
    "    val_sub = (test_sub + 1) % n_subjects\n",
    "    print(f\"Val Subject {val_sub + 1}:\")\n",
    "\n",
    "    # Initialize model and get source data for this iteration\n",
    "    model, parameters, loss_functions = initialize_model(test_sub, val_sub, n_subjects)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = get_source_data(test_sub, val_sub, n_subjects)\n",
    "    train_loader, val_loader = prepare_dataloaders(X_train, y_train, X_val, y_val, parameters['batch_size'])\n",
    "    test_loader = prepare_dataloaders(X_test, y_test, X_test, y_test, parameters['batch_size'])[1]  # Only need test loader\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters['lr'], betas=(parameters['b1'], parameters['b2']))\n",
    "\n",
    "    # Train the model\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.01)\n",
    "    trained_model, train_losses, val_losses, val_accuracies = train_model(model, optimizer, loss_functions, train_loader, val_loader, parameters, X_train, y_train, early_stopping)\n",
    "    \n",
    "    # Number of epochs trained is either the total number of epochs or until early stopping\n",
    "    epochs_trained = parameters['n_epochs'] if not early_stopping.early_stop else early_stopping.counter\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Test the model\n",
    "    test_accuracy, test_loss = test_model(trained_model, loss_functions, test_loader)\n",
    "\n",
    "    # Store results\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "    all_test_losses.append(test_loss)\n",
    "\n",
    "    print(f\"Test Subject {test_sub + 1}: Test Acc = {test_accuracy:.2f}%, Test Loss = {test_loss:.4f}\")\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed%60))\n",
    "    print('\\n======================================')\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results_df.loc[len(results_df)] = {\n",
    "        'Test Subject': test_sub + 1,\n",
    "        'Val Subject' : val_sub + 1,\n",
    "        'Test Acc'    : test_accuracy,\n",
    "        'Seed'        : seed_n\n",
    "}\n",
    "\n",
    "# Calculate average performance across all LOSO rounds\n",
    "average_accuracy = np.mean(all_test_accuracies)\n",
    "average_loss = np.mean(all_test_losses)\n",
    "\n",
    "print(f\"Average Test Accuracy: {average_accuracy:.2f}%\")\n",
    "print(f\"Average Test Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "    :param model: The trained model.\n",
    "    :param path: File path to save the model.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(path):\n",
    "    \"\"\"\n",
    "    Load a model from a file.\n",
    "    :param path: File path to the model.\n",
    "    :return: Loaded model.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = model_class(*args, **kwargs)\n",
    "    \n",
    "    # Load the model state dict\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(trained_model, 'results_2024_conf/model_cct.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loaded_model = load_model('results_2024_conf/model_cct.pth', ['left_hand', 'right_hand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegcct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
