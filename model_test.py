#!/usr/bin/env python3
"""
MSFAET Model Validation Script
æ–°ã—ã„MSFAETãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªå‹•ä½œç¢ºèªã¨CCTã¨ã®æ¯”è¼ƒ
"""

import torch
import torch.nn as nn
import numpy as np
from model.cct import CCT
from model.msfaet import MSFAET
import time

def test_model_forward_pass():
    """ãƒ¢ãƒ‡ãƒ«ã®å‰å‘ãæ¨è«–ãƒ†ã‚¹ãƒˆ"""
    print("=== ãƒ¢ãƒ‡ãƒ«å‰å‘ãæ¨è«–ãƒ†ã‚¹ãƒˆ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆBCI Competition IV Dataset 2aå½¢å¼ï¼‰
    batch_size = 16
    n_channels = 22
    n_samples = 1000
    n_classes = 2
    
    # ãƒ©ãƒ³ãƒ€ãƒ EEGãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    X = torch.randn(batch_size, 1, n_channels, n_samples).to(device)
    print(f"Input shape: {X.shape}")
    
    # CCTãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
    print("\n--- CCT Model Test ---")
    cct_model = CCT(
        kernel_sizes=[(22, 1), (1, 24)], stride=(1, 1), padding=(0, 0),
        pooling_kernel_size=(3, 3), pooling_stride=(1, 1), pooling_padding=(0, 0),
        n_conv_layers=2, n_input_channels=1, in_planes=64, activation=None,
        max_pool=False, conv_bias=False, dim=64, num_layers=4, num_heads=8, num_classes=2,
        attn_dropout=0.1, dropout=0.1, mlp_size=64, positional_emb="learnable"
    ).to(device)
    
    start_time = time.time()
    with torch.no_grad():
        cct_output = cct_model(X)
    cct_time = time.time() - start_time
    
    print(f"CCT Output shape: {cct_output.shape}")
    print(f"CCT Inference time: {cct_time:.4f}s")
    print(f"CCT Parameters: {sum(p.numel() for p in cct_model.parameters()):,}")
    
    # MSFAETãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
    print("\n--- MSFAET Model Test ---")
    msfaet_model = MSFAET(
        n_channels=22, n_classes=2, dim=64, depth=4, num_heads=8,
        mlp_ratio=4., qkv_bias=True, drop_rate=0.1, attn_drop_rate=0.1
    ).to(device)
    
    start_time = time.time()
    with torch.no_grad():
        msfaet_output = msfaet_model(X)
    msfaet_time = time.time() - start_time
    
    print(f"MSFAET Output shape: {msfaet_output.shape}")
    print(f"MSFAET Inference time: {msfaet_time:.4f}s")
    print(f"MSFAET Parameters: {sum(p.numel() for p in msfaet_model.parameters()):,}")
    
    # æ¯”è¼ƒ
    print("\n--- Comparison ---")
    speed_improvement = ((cct_time - msfaet_time) / cct_time) * 100
    param_difference = sum(p.numel() for p in msfaet_model.parameters()) - sum(p.numel() for p in cct_model.parameters())
    
    print(f"Speed improvement: {speed_improvement:+.1f}%")
    print(f"Parameter difference: {param_difference:+,}")
    
    # å‡ºåŠ›ã®æ¤œè¨¼
    assert cct_output.shape == (batch_size, n_classes), f"CCT output shape mismatch: {cct_output.shape}"
    assert msfaet_output.shape == (batch_size, n_classes), f"MSFAET output shape mismatch: {msfaet_output.shape}"
    
    print("\nâœ… å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")


def test_frequency_tokenizer():
    """å‘¨æ³¢æ•°èªè­˜Tokenizerã®å‹•ä½œç¢ºèª"""
    print("\n=== å‘¨æ³¢æ•°èªè­˜Tokenizerãƒ†ã‚¹ãƒˆ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    from model.msfaet import FrequencyAwareTokenizer
    
    tokenizer = FrequencyAwareTokenizer(dim=64, n_channels=22).to(device)
    
    # ã‚µãƒ³ãƒ—ãƒ«EEGãƒ‡ãƒ¼ã‚¿
    X = torch.randn(8, 1, 22, 1000).to(device)
    
    with torch.no_grad():
        tokens = tokenizer(X)
    
    print(f"Input shape: {X.shape}")
    print(f"Tokenized shape: {tokens.shape}")
    print(f"Sequence length: {tokens.shape[1]}")
    print(f"Embedding dimension: {tokens.shape[2]}")
    
    # å‘¨æ³¢æ•°å¸¯åŸŸã®ç¢ºèª
    print("\nå‘¨æ³¢æ•°å¸¯åŸŸè¨­å®š:")
    for name, (low, high) in tokenizer.freq_bands.items():
        print(f"  {name}: {low}-{high} Hz")
    
    print("âœ… Tokenizerãƒ†ã‚¹ãƒˆæˆåŠŸï¼")


def test_gradient_flow():
    """å‹¾é…ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== å‹¾é…ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ ===")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    model = MSFAET(
        n_channels=22, n_classes=2, dim=64, depth=2, num_heads=4,
        mlp_ratio=2., qkv_bias=True, drop_rate=0.0, attn_drop_rate=0.0
    ).to(device)
    
    # å°ã•ãªãƒãƒƒãƒã§è¨“ç·´ãƒ†ã‚¹ãƒˆ
    X = torch.randn(4, 1, 22, 1000).to(device)
    y = torch.randint(0, 2, (4,)).to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
    model.train()
    for i in range(5):
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        
        # å‹¾é…ã®ç¢ºèª
        total_norm = 0
        param_count = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        total_norm = total_norm ** (1. / 2)
        
        optimizer.step()
        
        print(f"Step {i+1}: Loss = {loss.item():.4f}, Grad norm = {total_norm:.4f}")
    
    print("âœ… å‹¾é…ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ MSFAET Model Validation Starting...")
    print("=" * 50)
    
    try:
        test_model_forward_pass()
        test_frequency_tokenizer() 
        test_gradient_flow()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("\nMSFAETãƒ¢ãƒ‡ãƒ«ã®ä¸»ãªç‰¹å¾´:")
        print("1. âœ… å‘¨æ³¢æ•°èªè­˜Tokenizer - EEGå„å‘¨æ³¢æ•°å¸¯åŸŸã‚’å€‹åˆ¥å‡¦ç†")
        print("2. âœ… åŠ¹ç‡çš„ãªLinear Attention - O(nÂ²)â†’O(n)ã®è¨ˆç®—é‡å‰Šæ¸›")
        print("3. âœ… é©å¿œçš„MLP - ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ©Ÿæ§‹ã«ã‚ˆã‚‹å‹•çš„å‡¦ç†")
        print("4. âœ… ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«å‡¦ç† - å±€æ‰€ãƒ»å¤§åŸŸç‰¹å¾´ã®çµ±åˆ")
        print("5. âœ… æ”¹è‰¯ãƒ—ãƒ¼ãƒªãƒ³ã‚° - CLSãƒˆãƒ¼ã‚¯ãƒ³ + æ³¨æ„æ©Ÿæ§‹")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 
